CSF Assignment 3 README

Vance Wood, vwood3
Daniel Webber, dwebber11


Best Cache:
The most common set, block and block size combination on the piazza forums was:
set number = 256, block/set = 4, block size = 16 bytes
So to find the best cache configuration, we're going to try different boolean parameters for these
constant numerical parameters. 
We're going to use gcc.trace as our test file, because that was also the most common one used on
Piazza, but also because it's the largest file, so we'll get the best data out of it. 
We have 6 configurations of cache to run and check. There are 3 boolean options which gives us
2^3 options, except we don't run the no-write-allocate write-back pair for either lru or fifo, so
2^ - 2 = 6 configuration.
Our goal is to find the best overall effectiveness, how will we determine this? Well misses is 
a good way to check, the more misses the worst, because this will lead to more cycles (100 each)
being needed and more time being needed to complete the query. 
We also want to see higher hit rates, which means items are found in our cache more often. 
Total chache size is another good metric for measuring cache performance.

./csim 256 4 16 write-allocate write-back fifo < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 314171
Load misses: 4026
Store hits: 188047
Store misses: 9439
Total cycles: 9831818


./csim 256 4 16 write-allocate write-through fifo < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 314171
Load misses: 4026
Store hits: 188047
Store misses: 9439
Total cycles: 25636818


./csim 256 4 16 no-write-allocate write-through fifo < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 311017
Load misses: 7180
Store hits: 163705
Store misses: 33781


./csim 256 4 16 write-allocate write-back lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 314798
Load misses: 3399
Store hits: 188250
Store misses: 9236
Total cycles: 9331848


./csim 256 4 16 write-allocate write-through lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 314798
Load misses: 3399
Store hits: 188250
Store misses: 9236
Total cycles: 25305648


./csim 256 4 16 no-write-allocate write-through lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 311613
Load misses: 6584
Store hits: 164819
Store misses: 32667
Total cycles: 22858632


First we're going to compare miss rates. 

We suspect that LRU will perform better than FIFO because 
it's more efficient for practical use. We can see this when comparing miss counts between LRU and FIFO.
We can see for WA WB lru and 3399 and 9236 load and store misses, while fifo had 4026 and 9439 misses. 
LRU had fewer misses, and fewer cycles. We can see this is true for the other WA/NWA and WT/WB pairs.
So we know LRU leads to fewer misses. 

We can see that regardless of WT/WB, when write-allocate is used with the same eviction policy, hit and 
miss rates are the same. Both WA LRU cases (the WB and the WT calls) returned:
Load hits: 314798
Load misses: 3399
Store hits: 188250
Store misses: 9236
And both WA FIFO cases (the WB and the WT calls) returned:
Load hits: 314171
Load misses: 4026
Store hits: 188047
Store misses: 9439
In both cases, cycles were much higher with WB, which is no surprise. In both cases, WB had ~9M cycles
while WT had ~25M. So we know write-back leads to fewer cycles. 

We can also see that no-write-allocate leads to about twice as many misses as write-allocate in all cases.
So we know that write-allocate leads to fewer misses.

So if we're trying to build a cache with the least cycles and least misses, write-allocate write-back lru 
is the way to go. 

Based on this, we wrote a python script to iterate over different size powers of two for our numerical
paramters and ran our simulation on them using write-allocate write-back lru. We had our script keep track 
of hits and cycles to report the configurations with the least cycles and most hits. 
We found that smaller block sizes are much better than large ones, we found 8 to be the optimal block size 
in our simulations.
Our best result was:
./csim 256 8 8 write-allocate write-back lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 313484
Load misses: 4713
Store hits: 179709
Store misses: 17777
Total cycles: 8471193

A close second was:
./csim 512 4 8 write-allocate write-back lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 313390
Load misses: 4807
Store hits: 179658
Store misses: 17828
Total cycles: 8511848

Here we can see 256 8 8 



Given the versatility of a fully associative caches, we decideed to test a fully associative cache of block
size 8. This lead to only 8444084, which was our best yet. 
./csim 1 2048 8 write-allocate write-back lru < gcc.trace
Total loads: 318197
Total stores: 197486
Load hits: 313533
Load misses: 4664
Store hits: 179751
Store misses: 17735
Total cycles: 8444084

But we know fully associative is infeasible to implement in hardware, so we've decided to choose the more 
feasible 256 8 8, set associative cache. 

So our idea cache configuration is 256 8 8 write-allocate write-back lru

Contributions:
We did most of the project together over zoom. Over 7 meetings and more than 10h on zoom we designed 
and wrote the overall data structures, methods, debugged, and tested our code. In our first meeting we 
started with block.h, set.h and chache.h to define the variables and methods. Then we 
wrote out the methods over a few meetings in the cpp files. Then once we finished the data structure
we debugged and tested for a couple of meetings. When debugging and testing, we zoomed and ran tests
seperately to maximize efficency. 

Individually, Vance wrote the initial cache_simulator.cpp file which took in the command line arguments, 
checked them for errors, and create the data structure. Between meetings he went threw the code and 
wrote all the comments, fixed bugs and warnings, code structure and redundancies. He also wrote the README

Individually, Daniel wrote the original make file, and he coded from his computer as he shared
screens while we zoomed. He had a few individual pushes aswell like fixing some bugs, and finishing
the set and cache class methods when I had to leave our meeting for another. He also fixed memory leaks.
He also wrote our python script to find best cache config.
Daniel definitely was the star of the team, guiding our discussion and typing while we zoomed. 